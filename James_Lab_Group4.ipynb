{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# James Lab Group 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Members: Hanlin Tian, Weizheng Wang, Ran Zhao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***After the represent the data, we transfer the column \"label\" to number 0 and 1. Then we tokenized the column \"title\" and \"text\" of making the iterator, then clean the data and transform them in the doc2vec. After that we split the data and put them into different set. Making a classifier of XGBoost, training the model and test it.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>title_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[ 1.1533764e-02  4.2144405e-03  1.9692603e-02 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[ 0.11267698  0.02518966 -0.00212591  0.021095...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[ 0.04253004  0.04300297  0.01848392  0.048672...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>[ 0.10801624  0.11583211  0.02874823  0.061732...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>[ 1.69016439e-02  7.13498285e-03 -7.81233795e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "4  It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                       title_vectors  \n",
       "0  [ 1.1533764e-02  4.2144405e-03  1.9692603e-02 ...  \n",
       "1  [ 0.11267698  0.02518966 -0.00212591  0.021095...  \n",
       "2  [ 0.04253004  0.04300297  0.01848392  0.048672...  \n",
       "3  [ 0.10801624  0.11583211  0.02874823  0.061732...  \n",
       "4  [ 1.69016439e-02  7.13498285e-03 -7.81233795e-...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from gensim import corpora\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "data = pd.read_csv('~/Documents/fake_news/FakeNewsTutorials/data/fake_or_real_news.csv', )\n",
    "labels=data['label']\n",
    "texts=data['text']\n",
    "titles=data['title']\n",
    "data.head()\n",
    "\n",
    "# Load in fake_or_real_news.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change the labels to 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarize(word):\n",
    "    if word  == 'REAL':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "Y = np.asarray(labels.apply(binarize)).reshape([labels.shape[0],])\n",
    "\n",
    "# binarize the labels, define fake real to 0, fake to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49944751381215469"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.sum()/len(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenized the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenized_title = [nltk.word_tokenize(title) for title in titles]\n",
    "tokenized_text = [nltk.word_tokenize(text) for text in texts]\n",
    "tokenized = tokenized_title\n",
    "for i in range(titles.shape[0]):\n",
    "    tokenized[i].extend(tokenized_text[i])\n",
    "    \n",
    "# making the iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In the first iterator, we do the clean data to delete the punctuations and other stopwords\n",
    "\n",
    "def clean_text(tokenized_list):\n",
    "    import string\n",
    "    sw = nltk.corpus.stopwords.words('english')\n",
    "    new_list = [[token.lower() for token in tlist if token not in string.punctuation and token.lower() not in sw] for tlist in tokenized_list]\n",
    "    return new_list\n",
    "cleaned = clean_text(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tagged_docs = [TaggedDocument(doc, tags=[idx]) for idx, doc in enumerate(cleaned)]\n",
    "doc2vec = Doc2Vec(vector_size=300, \n",
    "                  window=5,\n",
    "                  min_count=5,\n",
    "                  dm = 1,\n",
    "                  epochs=5)\n",
    "# dm, vector_size, dm=1, window, alpha, epochs, min_count  \n",
    "\n",
    "doc2vec.build_vocab(tagged_docs)\n",
    "doc2vec.train(tagged_docs, epochs=10, total_examples=doc2vec.corpus_count)\n",
    "\n",
    "# epochs=None, start_alpha=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(titles.shape[0]):\n",
    "     X.append(doc2vec.docvecs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We split the data into different dataset\n",
    "\n",
    "validation_size = .25\n",
    "test_size = .25\n",
    "validation_test_size = validation_size + test_size\n",
    "test_size_adjusted = test_size / validation_test_size\n",
    "\n",
    "X_train, X_test, Y_train, Y_test =  train_test_split(X, Y,test_size = validation_test_size)\n",
    "\n",
    "# X_validation, X_test, Y_validation, Y_test = train_test_split(X_validation_test, y_validation_test,test_size = test_size_adjusted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:linear',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We build the model by using the XGBoost\n",
    "\n",
    "from sklearn import ensemble\n",
    "model_xg = ensemble.GradientBoostingClassifier()\n",
    "model_xg.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "       REAL       0.92      0.89      0.90      1604\n",
      "       FAKE       0.89      0.92      0.90      1564\n",
      "\n",
      "avg / total       0.90      0.90      0.90      3168\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_pred = model_xg.predict(X_test)\n",
    "print(metrics.classification_report(Y_test, Y_pred, target_names=['REAL', 'FAKE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this prgram, we did several works:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Preparing the data and cleaning the data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Binarizing the label and Getting the Doc2Vec presentation of the text and title;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Making the XGBoost classifier model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Training the model and test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first iterator of the pipeline, the dataset had been cleaned for deleting the puntuations and stopwords, also fixing all the text-cap problem. For improving the performance of the pipeline, it could be done by combining the title vectors and text vectors, making multiple dimentional vectorization to fix the model. Also, we could tune the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used both the title and text to represent and transformed them to doc2vec representation. In the pipeline and hypertune process, learning rate is important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we did not test whether the given data is balaced or not, we could use the matplotlib and seaborn for making a graph to see the balance condition of the data. We also could tune the parameter of XGBoost or build a new classifier to evaluate the performance. If the data is imbalanced, we could do the resampling, copy the minority sample when the data is not enough (oversampling) or delete the majority sample when the data is too large to keep balanced.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
